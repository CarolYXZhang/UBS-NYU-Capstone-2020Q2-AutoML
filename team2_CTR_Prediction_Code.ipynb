{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "team2_CTR_Prediction_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfksz_uoVkVA",
        "colab_type": "text"
      },
      "source": [
        "# **Team 2: CTR Prdiction**\n",
        "Lei Guo (lg3175@nyu.edu), Xiangjun Kong (xk321@nyu.edu)\n",
        "\n",
        "- Please run this notebook using Google Colab (https://colab.research.google.com/)\n",
        "- Sign in to Google with your edu.com account so you can use the unlimited storage on Google Drive\n",
        "- Select **Runtime** in the tab and then **Change runtime type** and select **GPU** as the hardware accelerator\n",
        "- The estimated storage to run DLRM on the Criteo Kaggle Dataset is 40 GB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eNyB2rAVt5h",
        "colab_type": "text"
      },
      "source": [
        "### **Replicating Results on the Criteo Kaggle Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhM1jP-KMwKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# then you can view your Google Drive in Files at the sidebar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwwCF18dP1pk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone DLRM code from Facebook Research's Github and install the dependency package\n",
        "# Change directory to your Google Drive\n",
        "%cd /content/gdrive/My\\ Drive/\n",
        "!git clone https://github.com/facebookresearch/dlrm.git\n",
        "!pip install onnx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wg_-JvONeTE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change directory\n",
        "%cd /content/gdrive/My\\ Drive/dlrm/input/\n",
        "# Download the Criteo Kaggle Dataset\n",
        "!wget https://s3-eu-west-1.amazonaws.com/kaggle-display-advertising-challenge-dataset/dac.tar.gz\n",
        "# Extract the train.txt from the .tar.gz file\n",
        "!tar -xvf dac.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo77WYc_vN4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change directory\n",
        "%cd /content/gdrive/My\\ Drive/dlrm\n",
        "# Create a new folder to save DLRM's output\n",
        "%mkdir -p ./output/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEE9htsCvxUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A sample run of the code, with a tiny model is shown below\n",
        "!python dlrm_s_pytorch.py --mini-batch-size=2 --data-size=6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY3k8BQ9vzy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A sample run of the code, with a tiny model in debug mode\n",
        "!python dlrm_s_pytorch.py --mini-batch-size=2 --data-size=6 --debug-mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN-14VjlQq2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Replicate the results of Facebook. \n",
        "#Save model in model.pt in output folder. Save the results on run_kaggle_pt.log. \n",
        "! python dlrm_s_pytorch.py --arch-sparse-feature-size=16 --arch-mlp-bot=\"13-512-256-64-16\" --arch-mlp-top=\"512-256-1\" --data-generation=dataset --data-set=kaggle --raw-data-file=./input/train.txt --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=1024 --print-time --test-mini-batch-size=16384 --test-num-workers=16 --use-gpu --save-model=./output/model.pt --test-freq=1024 --memory-map 2>&1 | tee run_kaggle_pt.log\n",
        "\n",
        "#Change the path of log file.\n",
        "!cp -avr ./run_kaggle_pt.log ./output/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WhTheq4ROxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As for the article of Facebook, it suggests to use shell file to run the code. But we think use dlrm_s_pytorch.py is enough.\n",
        "# If you want to try to run dlrm_s_criteo_kaggle.sh, you may need to debug for \"Permission denied\". Use the code below.\n",
        "# !chmod u+x ./dlrm_s_criteo_kaggle.sh\n",
        "# ! ./dlrm_s_criteo_kaggle.sh --test-freq=1024 --use-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChceitqPACY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If you need to download datasets from kaggle, you can use the code below. Take avazu as an example.\n",
        "\n",
        "# !mkdir -p ~/.kaggle \n",
        "# !cp kaggle.json ~/.kaggle/ \n",
        "# !chmod 600 ~/.kaggle/kaggle.json \n",
        "\n",
        "# !kaggle competitions download -c avazu-ctr-prediction -p /content/gdrive/My\\ Drive/\n",
        "\n",
        "\n",
        "\n",
        "# If you want to upload datasets from your computer, you can use the code below.\n",
        "# from google.colab import files\n",
        "# files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYZmVCoOWCZo",
        "colab_type": "text"
      },
      "source": [
        "### **DLRM VS AutoML**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KTE5fE1ZDDv",
        "colab_type": "text"
      },
      "source": [
        "**Prepared data for SparkBeyond**  \n",
        "The following code converts 7 .npz files into .txt files. The you can download all .txt files and upload them to SparkBeyond to run AutoML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_tUJEZaSCQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform the pre-processed data(eg.train_day_0_processed.npz) into new txt files.\n",
        "# We will get 7 txt file each of which represents one day.\n",
        "import numpy as np\n",
        "\n",
        "def transform(num):\n",
        "  data = np.load('/content/gdrive/My Drive/dlrm/input/train_day_{0}_processed.npz'.format(num)) #change path here\n",
        "  X_cat = data['X_cat']\n",
        "  X_int = data['X_int']\n",
        "  y = data['y']\n",
        "  with open('train_day_{0}_sb.txt'.format(num),'w+') as f:\n",
        "    for i in range(len(y)):\n",
        "      line = ';'.join([str(y[i])]+[str(x) for x in X_int[i]]+[str(int(x)) for x in X_cat[i]])\n",
        "      f.writelines(line+'\\n')\n",
        "    f.close()\n",
        "\n",
        "for num_ in range(7):\n",
        "  print('working on: {0}'.format(num_))\n",
        "  transform(num_)\n",
        "  print('finished: {0}'.format(num_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTIu7BkNZNQq",
        "colab_type": "text"
      },
      "source": [
        "**Prepared data for H2O**  \n",
        "The following code converts seven .npz files into one .txt files. The you can download the .txt file and upload them to H2O to run AutoML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TBO4kAtUApP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The training set for H2O includes day 0 to day 5. Therefore, we need to combine train_day_0_sb.txt to train_day_5_sb.txt\n",
        "#Put these 6 files into a new folder.\n",
        "import os\n",
        "filedir = os.getcwd()+'/processed'\n",
        "filenames=os.listdir(filedir)\n",
        "f=open('processed_h2o/train_all.txt','w')\n",
        "#Go through all files in folder\n",
        "for filename in filenames:\n",
        "    filepath = filedir+'/'+filename\n",
        "    #Go through a file, write lines\n",
        "    for line in open(filepath):\n",
        "        f.writelines(line)\n",
        "# f.write('\\n')\n",
        "\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw16c9qvVYMD",
        "colab_type": "text"
      },
      "source": [
        "### **More on DLRM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNjREVjdWgoN",
        "colab_type": "text"
      },
      "source": [
        "**Imbalanced Data**\n",
        "\n",
        "- We modify corresponding  code in data_util.py. Then get *data_util_modify.py*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi6woHvZW2Vj",
        "colab_type": "text"
      },
      "source": [
        "**Run DLRM on Different Datasets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXFzXoJEMdcC",
        "colab_type": "text"
      },
      "source": [
        "- Add 3 arguments: --dense-count, --sparse-count, --day-count. Edit data_util.py / dlrm_data_pytorch.py / dlrm_s_pytorch.py to *data_util_modify.py / dlrm_data_pytorch_modify.py / dlrm_s_pytorch_modify.py*.\n",
        "\n",
        "- Put them all in the dlrm folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNne3JlsW_Yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Test new codes.\n",
        "#Choose smaller sample on the dataset to reduce the running time.\n",
        "#Because our target is only try to test whether DLRM can be run on other datasets. \n",
        "target_file = open('/content/gdrive/My Drive/dlrm/train2.txt', 'w+') # Change path to google drive\n",
        "source_file = open('/content/gdrive/My Drive/dlrm/input/train.txt') # Change your own path\n",
        "\n",
        "# 1,000,000 lines ~ 240 MB\n",
        "threshold = 1000000\n",
        "i, j = 0, 0\n",
        "\n",
        "for line in source_file:\n",
        "  if i>threshold: #Keep the first n lines\n",
        "    break\n",
        "  target_file.write(line)\n",
        "  #print(i,line)\n",
        "  i += 1 # threshold\n",
        "  if line[0]=='1':\n",
        "    j += 1 # The number of label=1 in new dataset\n",
        "\n",
        "source_file.close()\n",
        "target_file.close()\n",
        "# print(j/i) # 0.2549487450"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v6d_xwCXGjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "sub_sample_rate = 0.5\n",
        "i, j, k = 0, 0, 0\n",
        "\n",
        "target_file = open('/content/gdrive/My Drive/dlrm/train2_random.txt', 'w+')#modify as your path\n",
        "source_file = open('/content/gdrive/My Drive/dlrm/train2.txt')#modify as your path\n",
        "\n",
        "for line in source_file:\n",
        "  i += 1 # The number of the orignal dataset\n",
        "  if random.random()>sub_sample_rate:\n",
        "    continue\n",
        "  k += 1 # The number of sampling \n",
        "  target_file.write(line)\n",
        "  if line[0]=='1':\n",
        "    j += 1 # The number of label=1 in new dataset\n",
        "\n",
        "source_file.close()\n",
        "target_file.close()\n",
        "# print(j/k) # 0.2557276234\n",
        "# print(k/i) # 0.5005124994"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owBQToyCXrli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Try to reduce columns,then dense features = 12, sparse features = 24\n",
        "target_file = open('./modify/input/train3.txt', 'w+')#modify as your path\n",
        "source_file = open('./modify/input/train2_random.txt')#modify as your path\n",
        "\n",
        "for line in source_file:\n",
        "  #transform to list, delete some columns\n",
        "  line_new = line.replace(\"\\n\",\"\").split(\"\\t\")\n",
        "  line_new = line_new[:-2]\n",
        "  del line_new[2]\n",
        "  #add \\n\n",
        "  line_new.append(\"\\n\")\n",
        "  #transform to string\n",
        "  str_line = '\\t'.join(line_new)\n",
        "  str_line = str_line.replace(\"\\t\\n\",\"\\n\")\n",
        "  target_file.write(str_line)\n",
        "\n",
        "source_file.close()\n",
        "target_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th3gx4ijXwDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#After modified\n",
        "! python dlrm_s_pytorch_modify.py --data-sub-sample-rate=0.8 --dense-count=12 --sparse-count=24 --day-count=6 --arch-sparse-feature-size=16 --arch-mlp-bot=\"12-512-256-64-16\" --arch-mlp-top=\"512-256-1\" --data-generation=dataset --data-set=kaggle --raw-data-file=./modify/input/train3.txt --processed-data-file=./input/kaggleAdDisplayChallenge_processed.npz --loss-function=bce --round-targets=True --learning-rate=0.1 --mini-batch-size=128 --print-freq=1024 --print-time --test-mini-batch-size=16384 --test-num-workers=16 --use-gpu --save-model=./modify/output/model_modify.pt --test-freq=1024 --memory-map 2>&1 | tee run_kaggle_pt.log"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}